# -*- coding: utf-8 -*-
"""Car_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UpFle2aGy-tFZb37GqR6vlXvNyHq3h4l
"""

from google.colab import files

# Upload your Kaggle API credentials JSON file
uploaded = files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d nehalbirla/vehicle-dataset-from-cardekho

!unzip /content/vehicle-dataset-from-cardekho.zip

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv("/content/CAR DETAILS FROM CAR DEKHO.csv")

dataset

data=pd.read_csv('/content/car data.csv')

data4=pd.read_csv('/content/car details v4.csv')

data

data4

data.head()

print(data['Seller_Type'].unique())
print(data['Transmission'].unique())
print(data['Owner'].unique())
print(data['Fuel_Type'].unique())

# check missing or null values
data.isnull().sum()

data.columns

df=data[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',
       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']]

df['cy']=2023

df['Number_Years']=df['cy']-df['Year']

df

#Dropping Unwanted Column
df.drop(['Year'],axis=1,inplace=True)

df.drop(['cy'],axis=1,inplace=True)

df.head()

# Using One Hot Encoder for categorical data
df=pd.get_dummies(df,drop_first=True)

df.head()

df.corr()

import seaborn as sns

sns.pairplot(df)

#Pairplot
sns.set_style("whitegrid");
sns.pairplot(iris, hue="Lebel",height=4)
plt.show();

import matplotlib.pyplot as plt
corrmat=df.corr()
top_corr_feature=corrmat.index
plt.figure(figsize=(20,20))

G=sns.heatmap(df[top_corr_feature].corr(),annot=True)





X=df.iloc[:,1:]
y=df.iloc[:,0]

X.head()

y.head()

## Feature Importance
from sklearn.ensemble import ExtraTreesRegressor
model=ExtraTreesRegressor()
model.fit(X,y)

print(model.feature_importances_)

#plot graph of feature importances for better visualization
feat_imp=pd.Series(model.feature_importances_,index=X.columns)
feat_imp.nlargest(5).plot(kind='barh')
plt.show()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

X_train.shape

y_train.shape

from sklearn.ensemble import RandomForestRegressor
rf_random=RandomForestRegressor()

###Hyperparameters
import numpy as np
n_est=[int(x) for x in np.linspace(start=100,stop=1200,num=12)]
print(n_est)

n_est=[int(x) for x in np.linspace(start=100,stop=1200,num=12)]
max_feature=['auto','sqrt']
max_depth=[int (x) for x in np.linspace(5,30,num=6)]
min_sample_split=[2,5,10,15,100]
min_sample_leaf=[1,25,10]

from sklearn.model_selection import RandomizedSearchCV

# Create the random grid
random_grid = {'n_estimators': n_est,
               'max_features': max_feature,
               'max_depth': max_depth,
               'min_samples_split': min_sample_split,
               'min_samples_leaf': min_sample_leaf}

print(random_grid)

# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestRegressor()

rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)

rf_random.fit(X_train,y_train)

rf_random.best_params_

rf_random.best_score_

predictions=rf_random.predict(X_test)

X

pre=rf_random.predict(X)

sns.distplot(y_test-predictions)

plt.scatter(y_test,predictions)

from sklearn import metrics

print('MAE:', metrics.mean_absolute_error(y_test, predictions))
print('MSE:', metrics.mean_squared_error(y_test, predictions))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))

import pickle
# open a file, where you ant to store the data
file = open('random_forest_regression_model.pkl', 'wb')

# dump information to that file
pickle.dump(rf_random, file)

from sklearn.metrics import r2_score
r2_score(y,pre)

# with this model we are getting 97 percentage of accuracy

